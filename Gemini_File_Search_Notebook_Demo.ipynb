{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Legal Documents AI Search"
      ],
      "metadata": {
        "id": "qKoGc-HZFjZa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT5rH_mJ4VFs",
        "outputId": "2f059588-b9dc-42a1-efd2-52a46d214338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.55.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai) (2.43.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate 10 dummy JSON files for legal documents, each containing fields like 'title', 'case_id', 'date', 'parties', 'summary', and 'document_type', and save them in the \"my_docs\" folder."
      ],
      "metadata": {
        "id": "lSRLZWHrG73x"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25dc2f44"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Ensure the 'my_docs' directory exists\n",
        "if not os.path.exists('my_docs'):\n",
        "    os.makedirs('my_docs')\n",
        "\n",
        "def generate_dummy_legal_document(doc_id):\n",
        "    parties = [\n",
        "        f\"Plaintiff_{random.randint(1, 100)}\",\n",
        "        f\"Defendant_{random.randint(1, 100)}\"\n",
        "    ]\n",
        "    document_types = [\"Complaint\", \"Motion\", \"Order\", \"Brief\", \"Judgment\"]\n",
        "\n",
        "    # Generate a random date within the last year\n",
        "    random_days = random.randint(1, 365)\n",
        "    date = (datetime.now() - timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    summary_templates = [\n",
        "        \"This document pertains to a dispute over {item} between {party1} and {party2}. The court is reviewing the evidence.\",\n",
        "        \"A summary judgment was requested in the case of {party1} v. {party2} concerning {issue}. The outcome is pending.\",\n",
        "        \"An appeal was filed by {party1} against {party2} regarding the previous ruling on {topic}.\",\n",
        "        \"This {document_type} outlines the terms of a settlement reached between {party1} and {party2} concerning {matter}.\",\n",
        "        \"The court issued an {document_type} in favor of {party1} regarding {subject} following extensive hearings.\"\n",
        "    ]\n",
        "    random_summary_template = random.choice(summary_templates)\n",
        "    summary = random_summary_template.format(\n",
        "        item=random.choice([\"contract terms\", \"property rights\", \"copyright infringement\", \"patent dispute\", \"personal injury\"]),\n",
        "        party1=parties[0],\n",
        "        party2=parties[1],\n",
        "        issue=random.choice([\"breach of contract\", \"negligence claim\", \"unfair competition\", \"custody battle\", \"fraudulent misrepresentation\"]),\n",
        "        topic=random.choice([\"asset division\", \"corporate governance\", \"environmental regulations\"]),\n",
        "        document_type=random.choice(document_types),\n",
        "        matter=random.choice([\"a real estate transaction\", \"employment dispute\", \"a product liability claim\"])\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"title\": f\"{random.choice(document_types)} - Case {doc_id}\",\n",
        "        \"case_id\": f\"CASE-{random.randint(10000, 99999)}-{doc_id}\",\n",
        "        \"date\": date,\n",
        "        \"parties\": parties,\n",
        "        \"summary\": summary,\n",
        "        \"document_type\": random.choice(document_types)\n",
        "    }\n",
        "\n",
        "# Generate 10 dummy JSON files\n",
        "for i in range(1, 11):\n",
        "    doc_data = generate_dummy_legal_document(i)\n",
        "    file_name = f\"my_docs/legal_document_{i}.json\"\n",
        "    with open(file_name, 'w') as f:\n",
        "        json.dump(doc_data, f, indent=4)\n",
        "    print(f\"Generated: {file_name}\")\n",
        "\n",
        "print(f\"\\nSuccessfully generated 10 dummy legal document JSON files in the '{os.path.abspath('my_docs')}' folder.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZMKQ4P84tyR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"xxxx\"\n",
        "API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
        "FOLDER_PATH = \"my_docs\"\n",
        "STORE_NAME = \"rooms_reference_store\"\n",
        "MODEL_ID = \"gemini-2.5-flash\"\n",
        "\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "def _get_metadata_logic(filename):\n",
        "    \"\"\"\n",
        "    Returns a simple dictionary.\n",
        "    \"\"\"\n",
        "    metadata = {\"status\": \"active\", \"uploaded_via\": \"script\"}\n",
        "\n",
        "    if \"invoice\" in filename.lower():\n",
        "        metadata[\"category\"] = \"finance\"\n",
        "    elif \"manual\" in filename.lower():\n",
        "        metadata[\"category\"] = \"technical\"\n",
        "    else:\n",
        "        metadata[\"category\"] = \"general\"\n",
        "\n",
        "    return metadata\n",
        "\n",
        "def upload_folder_and_get_ids(store_name, folder_path):\n",
        "    files = glob.glob(os.path.join(folder_path, \"*.*\"))\n",
        "    valid_files = [f for f in files if f.endswith(('.txt', '.pdf', '.csv', '.md', '.json'))]\n",
        "\n",
        "    database_records = {}\n",
        "\n",
        "    if not valid_files:\n",
        "        print(\"No files found.\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Uploading {len(valid_files)} files with metadata...\")\n",
        "\n",
        "    for file_path in valid_files:\n",
        "        filename = os.path.basename(file_path)\n",
        "\n",
        "        # 1. Get the simple dict\n",
        "        raw_meta = _get_metadata_logic(filename)\n",
        "\n",
        "        # 2. CONVERT DICT TO LIST OF CustomMetadata OBJECTS\n",
        "        # FIX: Use 'string_value' instead of 'value'\n",
        "        formatted_metadata = [\n",
        "            types.CustomMetadata(key=k, string_value=str(v))\n",
        "            for k, v in raw_meta.items()\n",
        "        ]\n",
        "\n",
        "        print(f\" > Processing: {filename} | Meta: {raw_meta}...\", end=\"\")\n",
        "\n",
        "        try:\n",
        "            operation = client.file_search_stores.upload_to_file_search_store(\n",
        "                file=file_path,\n",
        "                file_search_store_name=store_name,\n",
        "                config={\n",
        "                    'display_name': filename,\n",
        "                    'custom_metadata': formatted_metadata\n",
        "                }\n",
        "            )\n",
        "\n",
        "            while not operation.done:\n",
        "                time.sleep(1)\n",
        "                operation = client.operations.get(operation)\n",
        "\n",
        "            if hasattr(operation, 'result') and operation.result:\n",
        "                doc_id = operation.result.name\n",
        "                database_records[filename] = doc_id\n",
        "                print(f\" [Indexed] -> ID: {doc_id}\")\n",
        "            else:\n",
        "                print(\" [Error: No ID returned]\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" [Upload Failed]: {e}\")\n",
        "\n",
        "    return database_records\n",
        "\n",
        "def delete_store_completely(store_name):\n",
        "    \"\"\"\n",
        "    Deletes the store and all its contents (documents/chunks) in one go.\n",
        "    Using force=True prevents 'FAILED_PRECONDITION' errors for non-empty stores.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Cleanup: Deleting Store {store_name} ---\")\n",
        "\n",
        "    try:\n",
        "        # The 'force' parameter tells Gemini to cascade delete all files/chunks\n",
        "        client.file_search_stores.delete(\n",
        "            name=store_name,\n",
        "            config={'force': True}\n",
        "        )\n",
        "        print(\"   Store and all contained files deleted successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Error during deletion: {e}\")\n",
        "\n",
        "\n",
        "def print_citations(response):\n",
        "    \"\"\"\n",
        "    Parses the response to extract and print source filenames.\n",
        "    \"\"\"\n",
        "    if not response.candidates or not response.candidates[0].grounding_metadata:\n",
        "        return\n",
        "\n",
        "    metadata = response.candidates[0].grounding_metadata\n",
        "\n",
        "    # 1. Collect all unique sources used\n",
        "    unique_sources = {}\n",
        "\n",
        "    if metadata.grounding_chunks:\n",
        "        for i, chunk in enumerate(metadata.grounding_chunks):\n",
        "            # For File Search, data is in 'retrieved_context'\n",
        "            if chunk.retrieved_context:\n",
        "                title = chunk.retrieved_context.title or \"Unknown File\"\n",
        "                uri = chunk.retrieved_context.uri\n",
        "                unique_sources[i] = {'title': title, 'uri': uri}\n",
        "\n",
        "    # 2. Map supports to the text (Optional: detailed inline citation)\n",
        "    # This part shows which sentence came from which file\n",
        "    if metadata.grounding_supports:\n",
        "        print(\"\\n\" + \"=\"*20 + \" CITATIONS \" + \"=\"*20)\n",
        "        for support in metadata.grounding_supports:\n",
        "            # The text segment that needs a citation\n",
        "            segment_text = support.segment.text if support.segment else \"Answer\"\n",
        "\n",
        "            # The indices of the chunks that support this segment\n",
        "            indices = support.grounding_chunk_indices\n",
        "            # print(indices)\n",
        "\n",
        "            if indices:\n",
        "                files = [unique_sources.get(idx, {}).get('title') for idx in indices]\n",
        "                # Filter out None values just in case\n",
        "                files = list(set(filter(None, files)))\n",
        "\n",
        "                if files:\n",
        "                    print(f\"üìù CLAIM: \\\"...{segment_text.strip()[:50]}...\\\"\")\n",
        "                    print(f\"   ‚Ü≥ SOURCE: {', '.join(files)}\")\n",
        "                    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clUHOBT6rJet"
      },
      "outputs": [],
      "source": [
        "# --- MAIN EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure dummy folder exists for testing\n",
        "    if not os.path.exists(FOLDER_PATH):\n",
        "        os.makedirs(FOLDER_PATH)\n",
        "        print(f\"Created '{FOLDER_PATH}'. Put files there and run again.\")\n",
        "        exit()\n",
        "\n",
        "    store_id = None\n",
        "    try:\n",
        "        # 1. Create Store\n",
        "        store = client.file_search_stores.create(config={'display_name': STORE_NAME})\n",
        "        store_id = store.name\n",
        "        print(f\"Store Created: {store_id}\")\n",
        "\n",
        "        # 2. Upload and Capture IDs for Database\n",
        "        # This returns the dictionary you requested\n",
        "        db_references = upload_folder_and_get_ids(store_id, FOLDER_PATH)\n",
        "\n",
        "        print(\"\\n--- IDs for your Database ---\")\n",
        "        for fname, fid in db_references.items():\n",
        "            print(f\"File: {fname} | Ref_ID: {fid}\")\n",
        "\n",
        "    finally:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoxOEw65j9MS"
      },
      "outputs": [],
      "source": [
        "# --- 5. Generate Content (RAG) ---\n",
        "question = \"<search keywork or user query>\"\n",
        "print(f\"\\nAsking: '{question}'...\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=question,\n",
        "            config=types.GenerateContentConfig(\n",
        "                tools=[\n",
        "                    types.Tool(\n",
        "                        file_search=types.FileSearch(\n",
        "                            file_search_store_names=[store.name],\n",
        "                            metadata_filter=\"status=active\",\n",
        "                        )\n",
        "                    )\n",
        "                ]\n",
        "              )\n",
        "          )\n",
        "\n",
        "# 4. Print Answer\n",
        "print(\"GEMINI ANSWER:\")\n",
        "print(response.text)\n",
        "\n",
        "# 5. Print Citations\n",
        "print_citations(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dss-qcf4jE1x"
      },
      "outputs": [],
      "source": [
        "# 4. Clean Delete (Fixes the 400 Error)\n",
        "if store_id:\n",
        "    delete_store_completely(store_id)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}